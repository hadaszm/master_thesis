{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from river.base.classifier import Classifier as RiverClassifer\n",
    "import queue\n",
    "import pandas as pd\n",
    "import typing\n",
    "import inspect\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import river\n",
    "import river.datasets as datasets\n",
    "from river import stream\n",
    "from pprint import pprint\n",
    "import os\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "from river import tree\n",
    "from river import evaluate\n",
    "from river import metrics\n",
    "\n",
    "import numpy as np\n",
    "from  streams.stream_section import StreamSection\n",
    "from streams.utils import FL, FU, generate_stream_section\n",
    "from my_datasets.getting_datasets import get_LED,get_Hyperplane,get_Electricity,get_AGRAWL,get_Airlines,get_CoverType,get_RandomRBF\n",
    "from train_and_eval import train_and_evaluate\n",
    "from functools import partial\n",
    "from train_and_eval import train_and_evaluate\n",
    "from river.metrics.accuracy import Accuracy\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from river.datasets.synth import ConceptDriftStream\n",
    "from tqdm import tqdm\n",
    "\n",
    "# methods\n",
    "from river.dummy import NoChangeClassifier,PriorClassifier\n",
    "from river.cluster import CluStream\n",
    "from river.forest.adaptive_random_forest import ARFClassifier\n",
    "from river.tree.hoeffding_adaptive_tree_classifier import  HoeffdingAdaptiveTreeClassifier\n",
    "from river.neighbors import KNNClassifier as riverKNN\n",
    "from river.naive_bayes import GaussianNB\n",
    "from semisupervised_methods.clustream_and_label import CluserAndLabel\n",
    "from semisupervised_methods.incremental_classifier import IncrementalClassifer\n",
    "from semisupervised_methods.one_nearest_neighbour import oneNNClassifer\n",
    "from river.drift import ADWIN, PageHinkley\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from streams.utils import FL, FU, generate_stream_section\n",
    "from streams.stream_section import StreamSection\n",
    "from river import metrics\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from constants import NUMBER_OF_THREADS, FREQUENCY_OF_PREDICTIONS\n",
    "import numpy as np\n",
    "import logging\n",
    "import datetime\n",
    "from river.utils import Rolling\n",
    "\n",
    "def get_most_frequent(predictions):\n",
    "    ''' \n",
    "    Get the most common predction\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    predictions: list\n",
    "        The list of predicted labels\n",
    "\n",
    "    Returns\n",
    "    ---------\n",
    "    int\n",
    "        The most popular class\n",
    "\n",
    "    '''\n",
    "    return max(set(predictions), key=predictions.count)\n",
    "\n",
    "\n",
    "def update_performance_measures(predictions, true_label, B, metrics):\n",
    "    ''' \n",
    "    The prediction for each awaiting (for a label) instance is made every K iterations \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    predictions: dict\n",
    "        The dictionary of predctions \n",
    "\n",
    "    true_label: int\n",
    "        The true class of an instance\n",
    "\n",
    "    B: int\n",
    "        Number of bins #TODO add somewhere the describtion\n",
    "\n",
    "    metrics: dict\n",
    "        The dictionary holding the metrics to calculate the results\n",
    "    '''\n",
    "    interval = (len(predictions)-1)/B\n",
    "    preds = list(predictions.values())\n",
    "    for b in range(B+2):\n",
    "        if b == 0:\n",
    "            metrics[b].update(true_label, preds[0])\n",
    "        if b == B+1:\n",
    "            metrics[b].update(true_label, preds[-1])\n",
    "        if preds[int(1+interval*(b)):int((1+interval*(b+1)))]: #interval not empty\n",
    "                y_pred = get_most_frequent(\n",
    "                    preds[int(1+interval*(b)):int((1+interval*(b+1)))])\n",
    "                metrics[b].update(true_label, y_pred)\n",
    "\n",
    "\n",
    "def make_prediction_for_awaiting(h, cur_idx, P, L, K):\n",
    "    ''' \n",
    "    The prediction for each awaiting (for a label) instance is made every K iterations \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    h: classifier\n",
    "        The trained classifer that predicts the class\n",
    "\n",
    "    cur_idx: int\n",
    "        The index of a currently processed observation\n",
    "\n",
    "    P: dict\n",
    "        The dictionary with predictions \n",
    "\n",
    "    L: dict\n",
    "        The dictionary with awaiting results\n",
    "\n",
    "    K: int\n",
    "        The number indicates how often the predction is made\n",
    "\n",
    "    '''\n",
    "    # foreach instance in L add predictionin time cur_idx in P\n",
    "    for idx, instance in L.items():\n",
    "        # make prediction every K instances\n",
    "        if abs(idx-cur_idx) % K == 0:\n",
    "            P[idx][cur_idx] = h.predict_one(instance)\n",
    "\n",
    "\n",
    "def add_delay_constant(stream, delay, no_delete_period, dataset_name, q):\n",
    "    new_stream = []\n",
    "    i = 0\n",
    "    for idx_1, idx_2, x, y, in stream:\n",
    "        if i < no_delete_period:\n",
    "            new_stream.append((i, i, x, y))\n",
    "            i = i+1\n",
    "            continue\n",
    "        if (i-no_delete_period) % delay == 0 and i-no_delete_period != 0:\n",
    "            i += delay\n",
    "        new_stream.append((i, i, x, None))\n",
    "        new_stream.append((i+delay, i, x, y))\n",
    "        i += 1\n",
    "    new_stream.sort(key=lambda x: x[0])\n",
    "    return StreamSection(f'{dataset_name}_delay_{q[0]}_{q[1]}', new_stream, False)\n",
    "\n",
    "def add_delay_random(stream, max_delay, no_delete_period, dataset_name, q):\n",
    "    \"\"\" Add random delay (1, max_delay)\"\"\"\n",
    "    new_stream = []\n",
    "    used_indexes = []\n",
    "    i = 0\n",
    "    for idx_1, idx_2, x, y, in stream:\n",
    "        if i < no_delete_period:\n",
    "            new_stream.append((i, i, x, y))\n",
    "            i = i+1\n",
    "            continue\n",
    "        while i in used_indexes:\n",
    "            i+=1\n",
    "        delay = np.random.randint(1,max_delay)\n",
    "        while i+delay in used_indexes:\n",
    "            delay+=1 # if sampling again infinite loop possible\n",
    "        new_stream.append((i, i, x, None))\n",
    "        new_stream.append((i+delay, i, x, y))\n",
    "        used_indexes.extend([i,i+delay])\n",
    "        i += 1\n",
    "    new_stream.sort(key=lambda x: x[0])\n",
    "    return StreamSection(f'{dataset_name}_delay_{q[0]}_{q[1]}', new_stream, False)\n",
    "\n",
    "\n",
    "def generate_streams(initial_stream, dataset_name, q, probas, delay_type, delay, warm_up_period):\n",
    "    stream_set = []  # TODO chcek where this should be placed\n",
    "\n",
    "    stream_set.append(initial_stream)\n",
    "    if delay_type == 1:\n",
    "        initial_stream = add_delay_constant(\n",
    "            initial_stream.stream, delay, warm_up_period, dataset_name, q)\n",
    "        dataset_name += '_constant_delay'\n",
    "        stream_set.append(initial_stream)\n",
    "    elif delay_type == 2:\n",
    "        initial_stream = add_delay_random(\n",
    "            initial_stream.stream, delay, warm_up_period, dataset_name, q)\n",
    "        dataset_name += '_random_delay'\n",
    "        stream_set.append(initial_stream)\n",
    "\n",
    "    for p in probas:\n",
    "        ssl_stream = StreamSection(f'{dataset_name}_ssl_{p}_{q[0]}_{q[1]}', FU(\n",
    "            initial_stream.stream, p, warm_up_period), False)\n",
    "        lfs_stream = StreamSection(\n",
    "            f'{dataset_name}_lfs_{p}_{q[0]}_{q[1]}', FL(ssl_stream.stream), True)\n",
    "        stream_set.append(ssl_stream)\n",
    "        stream_set.append(lfs_stream)\n",
    "    return stream_set\n",
    "\n",
    "\n",
    "\n",
    "def train_for_stream( my_stream,methods, methods_params,methods_name,\n",
    "                       metric_fun, K, B,warm_up_period, max_length):\n",
    "    results = {}\n",
    "    predictions = {}\n",
    "    # how many labelled instances appered -> needed for prediction of awaiting examples \n",
    "    labelled_insances_cnt = 0\n",
    "    logging.debug(f\" Start processing {my_stream.__name__}\")\n",
    "    for mi, method in enumerate(methods):\n",
    "            # initilaze method and variables\n",
    "        m = method(**methods_params[mi])\n",
    "        metrics = [metric_fun() for _ in range(B+2)]\n",
    "        periodc_metric = Rolling(metric_fun(),window_size = FREQUENCY_OF_PREDICTIONS )\n",
    "        final_pred_history = []\n",
    "        h = m\n",
    "        L = {}\n",
    "        P = {}\n",
    "        preds = []\n",
    "        logging.debug(f\" Start processing method {methods_name[mi]}\")\n",
    "        for cur_idx, init_idx, x, y in my_stream.stream:\n",
    "            # TODO: can it be in this place\n",
    "           \n",
    "            # unlabelled instance\n",
    "            if y is None:\n",
    "                # add instnace and index\n",
    "                L[cur_idx] = x\n",
    "                P[cur_idx] = {}\n",
    "                # predict if after warm up period\n",
    "                if m._timestamp > warm_up_period:\n",
    "                    P[cur_idx][cur_idx] = h.predict_one(x)\n",
    "                    # TODO: think what to do if the method cannot deal with unlabelled\n",
    "                    preds.append(max(h.predict_proba_one(\n",
    "                    x).items(), key=operator.itemgetter(1))[1])\n",
    "                    h = h.learn_one(x)\n",
    "\n",
    "            # labelled instance\n",
    "            else:\n",
    "                labelled_insances_cnt+=1\n",
    "                periodc_metric.update(y, h.predict_one(x))\n",
    "                if cur_idx != init_idx and init_idx in P.keys():  # delayed label\n",
    "                    P[init_idx][cur_idx] = h.predict_one(x)\n",
    "                    L.pop(init_idx)\n",
    "\n",
    "                    update_performance_measures(\n",
    "                        P[init_idx], y, B, metrics)\n",
    "                # TODO: probably need to implment a better option of evaluation-> for now test then train is used\n",
    "                else: #TODO: dopytac sie czy tojest ok \n",
    "                    if m._timestamp > warm_up_period:  # if in warmup period the prediction cannot be made\n",
    "                        # if it was not delayed only last prediction exists\n",
    "                        metrics[B+1].update(y, h.predict_one(x))\n",
    "                        \n",
    "                \n",
    "                make_prediction_for_awaiting(h, labelled_insances_cnt, P, L, K) \n",
    "                h = h.learn_one(x, y)\n",
    "                if labelled_insances_cnt %FREQUENCY_OF_PREDICTIONS == 0 and labelled_insances_cnt>0 :\n",
    "                    final_pred_history.append(periodc_metric.get())\n",
    "        method_params = ' '.join([f\"({k};{v})\" for k,v in m._get_params().items()])\n",
    "        predictions_through_time = [str(final_pred_history[t])  if t< len(final_pred_history) else \"\" for t in range(max_length)]\n",
    "    \n",
    "\n",
    "        predictions_through_time = ', '.join(predictions_through_time)\n",
    "        logging.info(f\"{my_stream.__name__}, {method_params}, {B},{FREQUENCY_OF_PREDICTIONS}, {', '.join([str(t.get())for t in metrics])},{predictions_through_time}\")        \n",
    "        results[methods_name[mi]] = metrics,final_pred_history,preds\n",
    "        \n",
    "    return my_stream.__name__,results\n",
    "\n",
    "# K how many new labelled instances need to arrive before the new prediction is made\n",
    "def train_and_evaluate(initial_stream, dataset_name, Q, probas, methods, methods_params,methods_name,\n",
    "                       metric_fun, delay_type, K, B, delay,warm_up_period=10):\n",
    "    '''\n",
    "    The initial stream section needs to passed\n",
    "    Main evaluation and traing function\n",
    "    delay_type - 0 - NONE, 1 - equal, 2- random\n",
    "    '''\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    date_time = now.strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "    logging.basicConfig(filename=f'logs\\\\{date_time}.log', filemode='w', format='%(asctime)s - %(message)s',level=logging.INFO,datefmt='%d-%b-%y %H:%M:%S')\n",
    "    pool = ThreadPool(NUMBER_OF_THREADS)\n",
    "    results = {}\n",
    "    for q in Q:\n",
    "        # preparing streams part\n",
    "        stream_set = generate_streams(\n",
    "            initial_stream, dataset_name, q, probas, delay_type, delay,warm_up_period,)\n",
    "        logging.debug('Streams generated')\n",
    "        # train and evaluation part\n",
    "        max_length = int(np.ceil(max([len(st.stream) for st in stream_set])/FREQUENCY_OF_PREDICTIONS))\n",
    "        results_for_q = []\n",
    " \n",
    "        for my_stream in stream_set[1:]:\n",
    "            results_for_q.append(train_for_stream( my_stream,methods, methods_params, methods_name,\n",
    "                       metric_fun, K, B,warm_up_period,max_length))\n",
    "            \n",
    "    \n",
    "            \n",
    "        results[q] = results_for_q\n",
    "    return results\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClassifier(RiverClassifer):\n",
    "    def __init__(self,classifer,clustering_method, classifier_params,clustering_params,time_window):\n",
    "        \"\"\"\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        classifer: RiverClassifer (probably)\n",
    "            base classifer, needs to have a predict_proba (TODO: chceck what if not)\n",
    "        \n",
    "        clustering_method: Sklearn style cliustering method\n",
    "            clustering method for unlabelled instances\n",
    "        \n",
    "        classifier_params: dict\n",
    "            parameters for the classifier\n",
    "\n",
    "        clustering_params: dict\n",
    "            parameters for the clustering method\n",
    "\n",
    "        time_window: int\n",
    "            after how many labelled instances the prediction on centers are made #TODO decide if I should count labelled or unlabelled instances\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.counter = 0 # counter of the labelled instances\n",
    "        self.classifier= classifer(**classifier_params)\n",
    "        self.clustering_method = clustering_method(**clustering_params)\n",
    "        self.time_window = time_window\n",
    "        self._timestamp = 0\n",
    "        self.unlabelled_instances = [] # I considered queue but since we need the whole chunk of data list of dicts\n",
    "        # seems to be more resonable\n",
    "\n",
    "    def _learn_from_unlabelled(self,columns):\n",
    "\n",
    "        # train the clustering method\n",
    "\n",
    "\n",
    "        X = pd.DataFrame.from_records(self.unlabelled_instances)\n",
    "        self.unlabelled_instances.clear()\n",
    "        if len(X) <4: # TODO: number of clusters\n",
    "            return\n",
    "        self.clustering_method = self.clustering_method.fit(X)\n",
    "\n",
    "        #TODO: rewrite vectorize\n",
    "        predicted_labels = []\n",
    "        for i,center in enumerate(self.clustering_method.cluster_centers_):\n",
    "            x = dict(zip(columns,center))\n",
    "            y_proba = self.classifier.predict_proba_one(x)\n",
    "            y = self.classifier.predict_one(x)\n",
    "            # print(i,y_proba)\n",
    "            predicted_labels.append((x,y))\n",
    "\n",
    "        for x,y in predicted_labels:\n",
    "            self._timestamp+=1\n",
    "            self.classifier = self.classifier.learn_one(x,y)\n",
    "\n",
    "    def learn_one(self,x,y=None):\n",
    "        if y is None:\n",
    "            self.unlabelled_instances.append(x)\n",
    "            return self\n",
    "        else:\n",
    "            self._timestamp+=1\n",
    "            self.classifier = self.classifier.learn_one(x=x, y=y)\n",
    "            if self._timestamp%self.time_window == 0:\n",
    "                self._learn_from_unlabelled(list(x.keys()))\n",
    "            return self\n",
    "\n",
    "    def predict_one(self, x):\n",
    "        return self.classifier.predict_one(x=x)\n",
    "\n",
    "    def predict_proba_one(self, x):\n",
    "        return self.classifier.predict_proba_one(x=x)\n",
    "\n",
    "\n",
    "    def _get_params(self) -> typing.Dict[str, typing.Any]:\n",
    "        \"\"\"Return the parameters that were used during initialization.\"\"\"\n",
    "\n",
    "        params = {}\n",
    "\n",
    "        for name, param in inspect.signature(self.classifier.__init__).parameters.items():  # type: ignore\n",
    "        \n",
    "            # Keywords parameters\n",
    "            attr = getattr(self.classifier, name)\n",
    "            params[f\"classifier_\"+ str(name)] = attr\n",
    "\n",
    "        for name, param in inspect.signature(self.clustering_method.__init__).parameters.items():  # type: ignore\n",
    "        \n",
    "            # Keywords parameters\n",
    "            attr = getattr(self.clustering_method, name)\n",
    "            params[f\"clustering_\"+ str(name)] = attr\n",
    "\n",
    "\n",
    "        params['clustering'] = self.clustering_method.__class__\n",
    "        params['classifier'] = self.classifier.__class__\n",
    "        params['time_window'] = self.time_window\n",
    "            \n",
    "\n",
    "        return params\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf = get_RandomRBF(change_speed=0.0001,n_centroids=50)\n",
    "initial_stream_r = generate_stream_section(rbf,'initail_RBF_moderate','initail_RBF_moderate',start=0,stop=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "led3 = get_LED(0.1,True,3)\n",
    "led5 = get_LED(0.1,True,5)\n",
    "led7 = get_LED(0.1,True,7)\n",
    "\n",
    "cds = ConceptDriftStream(ConceptDriftStream(led3,led5,position=3750,width = 3750),ConceptDriftStream(led5,led7,position=7500,width = 5000), position=7500,width=7500)\n",
    "initial_stream_l = generate_stream_section(cds,'initail_LED_Drift_gradual','initail_LED_Drift_gradual',start=0,stop=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'LED_gradual'\n",
    "Q=[(0,20000)]\n",
    "probas = [0.1,0.2,0.5]\n",
    "ARF1 = ARFClassifier\n",
    "ARF2 = ARFClassifier\n",
    "ARF3 = ARFClassifier\n",
    "NB1 = GaussianNB\n",
    "NB3 = GaussianNB\n",
    "NB2 = GaussianNB\n",
    "incClasif1 = IncrementalClassifer\n",
    "incClasif2= IncrementalClassifer\n",
    "incClasif3 = IncrementalClassifer\n",
    "incClasif4 = IncrementalClassifer\n",
    "incClasif5= IncrementalClassifer\n",
    "incClasif6 = IncrementalClassifer\n",
    "m_params = [\n",
    " {'threshold':0.9,\n",
    "  'classifier':ARF1,\n",
    "  'params':{'seed':123}},\n",
    "  {'threshold':0.8,\n",
    "  'classifier':ARF2,\n",
    "  'params':{'seed':123}},\n",
    "  {'threshold':0.7,\n",
    "  'classifier':ARF3,\n",
    "  'params':{'seed':123}},\n",
    "  {'threshold':0.9,\n",
    "  'classifier':NB1,\n",
    "  'params':{}},\n",
    "  {'threshold':0.8,\n",
    "  'classifier':NB2,\n",
    "  'params':{}},\n",
    "  {'threshold':0.7,\n",
    "  'classifier':NB3,\n",
    "  'params':{}}]\n",
    "methods = [incClasif1,incClasif2,incClasif3,incClasif4,incClasif5,incClasif6]\n",
    "resl= train_and_evaluate(initial_stream_l,dataset_name,Q,probas,methods,m_params,['ARF','ARF','ARF','NB','NB','NB'],Accuracy,1,B=50,K=10,delay=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'RBF_moderate'\n",
    "Q=[(0,20000)]\n",
    "probas = [0.1,0.2,0.5]\n",
    "ARF1 = ARFClassifier\n",
    "ARF2 = ARFClassifier\n",
    "ARF3 = ARFClassifier\n",
    "NB1 = GaussianNB\n",
    "NB3 = GaussianNB\n",
    "NB2 = GaussianNB\n",
    "incClasif1 = IncrementalClassifer\n",
    "incClasif2= IncrementalClassifer\n",
    "incClasif3 = IncrementalClassifer\n",
    "incClasif4 = IncrementalClassifer\n",
    "incClasif5= IncrementalClassifer\n",
    "incClasif6 = IncrementalClassifer\n",
    "m_params = [\n",
    " {'threshold':0.9,\n",
    "  'classifier':ARF1,\n",
    "  'params':{'seed':123}},\n",
    "  {'threshold':0.8,\n",
    "  'classifier':ARF2,\n",
    "  'params':{'seed':123}},\n",
    "  {'threshold':0.7,\n",
    "  'classifier':ARF3,\n",
    "  'params':{'seed':123}},\n",
    "  {'threshold':0.9,\n",
    "  'classifier':NB1,\n",
    "  'params':{}},\n",
    "  {'threshold':0.8,\n",
    "  'classifier':NB2,\n",
    "  'params':{}},\n",
    "  {'threshold':0.7,\n",
    "  'classifier':NB3,\n",
    "  'params':{}}]\n",
    "methods = [incClasif1,incClasif2,incClasif3,incClasif4,incClasif5,incClasif6]\n",
    "resr = train_and_evaluate(initial_stream_r,dataset_name,Q,probas,methods,m_params,['ARF','ARF','ARF','NB','NB','NB'],Accuracy,1,B=50,K=10,delay=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('RBF_moderate_delay_0_20000',\n",
       " {'ARF': ([Accuracy: 61.31%,\n",
       "    Accuracy: 0.00%,\n",
       "    Accuracy: 0.00%,\n",
       "    Accuracy: 0.00%,\n",
       "    Accuracy: 64.44%,\n",
       "    Accuracy: 61.50%,\n",
       "    Accuracy: 67.00%,\n",
       "    Accuracy: 65.50%,\n",
       "    Accuracy: 58.00%,\n",
       "    Accuracy: 62.50%,\n",
       "    Accuracy: 0.00%,\n",
       "    Accuracy: 62.00%,\n",
       "    Accuracy: 63.75%,\n",
       "    Accuracy: 71.60%,\n",
       "    Accuracy: 62.81%,\n",
       "    Accuracy: 0.00%,\n",
       "    Accuracy: 59.83%,\n",
       "    Accuracy: 0.00%,\n",
       "    Accuracy: 68.51%,\n",
       "    Accuracy: 59.30%,\n",
       "    Accuracy: 0.00%,\n",
       "    Accuracy: 65.50%,\n",
       "    Accuracy: 65.19%,\n",
       "    Accuracy: 0.00%,\n",
       "    Accuracy: 60.32%,\n",
       "    Accuracy: 0.00%,\n",
       "    Accuracy: 0.00%,\n",
       "    Accuracy: 65.47%,\n",
       "    Accuracy: 64.50%,\n",
       "    Accuracy: 59.55%,\n",
       "    Accuracy: 0.00%,\n",
       "    Accuracy: 68.51%,\n",
       "    Accuracy: 0.00%,\n",
       "    Accuracy: 60.17%,\n",
       "    Accuracy: 61.11%,\n",
       "    Accuracy: 64.50%,\n",
       "    Accuracy: 70.99%,\n",
       "    Accuracy: 63.25%,\n",
       "    Accuracy: 62.50%,\n",
       "    Accuracy: 59.55%,\n",
       "    Accuracy: 71.60%,\n",
       "    Accuracy: 59.00%,\n",
       "    Accuracy: 65.00%,\n",
       "    Accuracy: 68.00%,\n",
       "    Accuracy: 61.06%,\n",
       "    Accuracy: 72.22%,\n",
       "    Accuracy: 0.00%,\n",
       "    Accuracy: 0.00%,\n",
       "    Accuracy: 0.00%,\n",
       "    Accuracy: 61.96%,\n",
       "    Accuracy: 0.00%,\n",
       "    Accuracy: 61.96%],\n",
       "   [0.1,\n",
       "    0.65,\n",
       "    0.71,\n",
       "    0.63,\n",
       "    0.62,\n",
       "    0.61,\n",
       "    0.65,\n",
       "    0.63,\n",
       "    0.61,\n",
       "    0.57,\n",
       "    0.69,\n",
       "    0.69,\n",
       "    0.66,\n",
       "    0.6,\n",
       "    0.65,\n",
       "    0.58,\n",
       "    0.61,\n",
       "    0.75,\n",
       "    0.72,\n",
       "    0.62],\n",
       "   [0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.30000000000000004,\n",
       "    0.95316442425878,\n",
       "    0.9032408791988358,\n",
       "    0.8643929345611819,\n",
       "    0.9631698144757787,\n",
       "    0.9890366697365496,\n",
       "    0.6202205909978727,\n",
       "    0.7017254353164005,\n",
       "    0.981944758783395,\n",
       "    0.927450214177631,\n",
       "    0.4656435415769867,\n",
       "    0.55391668486908,\n",
       "    0.89694024931476,\n",
       "    0.9500110856588097,\n",
       "    0.6142686574198423,\n",
       "    0.5296485312000704,\n",
       "    0.9011191306500015,\n",
       "    0.8172896880363545,\n",
       "    0.5427311239235321,\n",
       "    0.5292901072523857,\n",
       "    0.9898600523593536,\n",
       "    0.8284798272064287,\n",
       "    0.9629267880864486,\n",
       "    0.9403977040955993,\n",
       "    0.36229570644612663,\n",
       "    0.8599922624080781,\n",
       "    0.8640853872602721,\n",
       "    0.7168598510015997,\n",
       "    0.8722499068341806,\n",
       "    0.962353506837606,\n",
       "    0.6971198450034477,\n",
       "    0.8505755585779533,\n",
       "    0.9932045293274983,\n",
       "    0.7980846577810373,\n",
       "    0.9186284831132803,\n",
       "    0.9090102895251352,\n",
       "    0.925256467861523,\n",
       "    0.4633075981465603,\n",
       "    0.9300783791692503,\n",
       "    0.9932892481784954,\n",
       "    0.9912379993147202,\n",
       "    0.889823506732686,\n",
       "    0.8359525042212101,\n",
       "    0.6806424647600036,\n",
       "    0.6357986442411203,\n",
       "    0.9823884009411826,\n",
       "    0.2912420295540153,\n",
       "    0.9999795565847336,\n",
       "    0.9674594164224563,\n",
       "    0.9684050690424814,\n",
       "    0.9818539553317986,\n",
       "    0.8967142975762605,\n",
       "    0.9890621780520772,\n",
       "    0.9897914247783599,\n",
       "    0.6477313490038161,\n",
       "    0.7627312530935099,\n",
       "    0.9470618684592146,\n",
       "    0.8292904418829316,\n",
       "    0.8831787802282185,\n",
       "    0.9997110360208256,\n",
       "    0.9854790182384211,\n",
       "    0.9465106519385615,\n",
       "    0.9989363939285405,\n",
       "    0.9987677218833189,\n",
       "    0.9979925196749774,\n",
       "    0.6758164198954264,\n",
       "    0.997426133496916,\n",
       "    0.996766270037824,\n",
       "    0.9072149554781966,\n",
       "    0.9982621103010487,\n",
       "    0.9738931337902806,\n",
       "    0.8027057249326067,\n",
       "    0.9985535000890933,\n",
       "    0.9970030174828456,\n",
       "    0.9603994540206365,\n",
       "    0.9973527395913451,\n",
       "    0.9988429497459814,\n",
       "    0.9285384210207415,\n",
       "    0.9733462521584263,\n",
       "    0.5300387965048671,\n",
       "    0.9999587710508157,\n",
       "    0.8426199217530359,\n",
       "    0.9833198576161093,\n",
       "    0.9843053023218731,\n",
       "    0.9995127458957876,\n",
       "    0.8501885341161871,\n",
       "    0.9801923662335167,\n",
       "    0.9985615376835812,\n",
       "    0.9981669068065306,\n",
       "    0.993719136575058,\n",
       "    0.9987127165036117,\n",
       "    0.99981271574193,\n",
       "    0.9996857940506277,\n",
       "    0.8468404943041212,\n",
       "    0.9999763187489621,\n",
       "    0.7261138230823811,\n",
       "    0.9299583227425983,\n",
       "    0.5675211444163877,\n",
       "    0.9094989938061865,\n",
       "    0.9996675220024692,\n",
       "    0.9982914260490817,\n",
       "    0.996333402075217,\n",
       "    0.9988153908957014,\n",
       "    0.4911630576558681,\n",
       "    0.9999983812704891,\n",
       "    0.8005355751309123,\n",
       "    0.708534550951856,\n",
       "    0.9960765949353108,\n",
       "    0.9999314990530872,\n",
       "    0.921646398190209,\n",
       "    0.9992790389801977,\n",
       "    0.9998958268030035,\n",
       "    0.9372623288022782,\n",
       "    0.7213691923147666,\n",
       "    0.8648080724625686,\n",
       "    0.9035191035063299,\n",
       "    0.5929331068186959,\n",
       "    0.8636030276242002,\n",
       "    0.6972504829422328,\n",
       "    0.9911169376968467,\n",
       "    0.9500522906203043,\n",
       "    0.9395993094498832,\n",
       "    0.9998832338366276,\n",
       "    0.9963195536659724,\n",
       "    0.5797719553435707,\n",
       "    0.8741716492055842,\n",
       "    0.9912025537853277,\n",
       "    0.9999465000145762,\n",
       "    0.9993800819864914,\n",
       "    0.9965542889590575,\n",
       "    0.9999974141307018,\n",
       "    0.5247954004290091,\n",
       "    0.755116805580227,\n",
       "    0.9972485056793852,\n",
       "    0.9982682401128862,\n",
       "    0.6494582487885052,\n",
       "    0.9981347001596944,\n",
       "    0.9223268892454977,\n",
       "    0.9918452214562291,\n",
       "    0.999896049417105,\n",
       "    0.9760133263613396,\n",
       "    0.9670449696545446,\n",
       "    0.9817872215881103,\n",
       "    0.8286005535787491,\n",
       "    0.9915905871519225,\n",
       "    0.9942891877441323,\n",
       "    0.9991037495077858,\n",
       "    0.9974948350085774,\n",
       "    0.9977979112235892,\n",
       "    0.991832501659671,\n",
       "    0.8531967444909238,\n",
       "    0.9950281284288113,\n",
       "    0.8400209890647442,\n",
       "    0.809101672012442,\n",
       "    0.9998058080647578,\n",
       "    0.9998500456651441,\n",
       "    0.9922452915907065,\n",
       "    0.9991512863209627,\n",
       "    0.9687038521389275,\n",
       "    0.8578637630997552,\n",
       "    0.3984622732950724,\n",
       "    0.999528049519005,\n",
       "    0.9998008586720895,\n",
       "    0.9487810652058617,\n",
       "    0.9999525673876203,\n",
       "    0.9994983040907628,\n",
       "    0.9909398081006734,\n",
       "    0.999741390491917,\n",
       "    0.7318760716879544,\n",
       "    0.9999653266429918,\n",
       "    0.7411931175276083,\n",
       "    0.9983175659466852,\n",
       "    0.9987862030050795,\n",
       "    0.9999675585632277,\n",
       "    0.9776248087205545,\n",
       "    0.9368921723738544,\n",
       "    0.9966203779454882,\n",
       "    0.5899808652166649,\n",
       "    0.9240408228414951,\n",
       "    0.9851454303343766,\n",
       "    0.9969655083758586,\n",
       "    0.9999880812850577,\n",
       "    0.9957599889055145,\n",
       "    0.9773917996972702,\n",
       "    0.9442032986299456,\n",
       "    0.9998946858782415,\n",
       "    0.9998738103612024,\n",
       "    0.9997825373534125,\n",
       "    0.7929848498939253,\n",
       "    0.999094221636143,\n",
       "    0.9769991451916883,\n",
       "    0.9999791977054294,\n",
       "    0.9993486053059174,\n",
       "    0.5384009053623474,\n",
       "    0.9692431195177359,\n",
       "    0.6759105327657272,\n",
       "    0.9999946001236136,\n",
       "    0.9967106635080452,\n",
       "    0.9989542248045101,\n",
       "    0.9999269202191112,\n",
       "    0.9917698675008241,\n",
       "    0.9954017834503301,\n",
       "    0.5693067554758913,\n",
       "    0.928706551312898,\n",
       "    0.7813048139326344,\n",
       "    0.5513428661222699,\n",
       "    0.9775155262946682,\n",
       "    0.9976832521033377,\n",
       "    0.804827954071791,\n",
       "    0.5234698887734719,\n",
       "    0.9832708937623055,\n",
       "    0.9973825360525097,\n",
       "    0.999051297993623,\n",
       "    0.9977354018045587,\n",
       "    0.8743811779833992,\n",
       "    0.9996993954202561,\n",
       "    0.9994308969424047,\n",
       "    0.9873052474795878,\n",
       "    0.9977306425305502,\n",
       "    0.5363452569581054,\n",
       "    0.5339204916821986,\n",
       "    0.9997893850804648,\n",
       "    0.9186600126705119,\n",
       "    0.9939631240526002,\n",
       "    0.9879299911696354,\n",
       "    0.9849664132869786,\n",
       "    0.9999621802634436,\n",
       "    0.9962210988903172,\n",
       "    0.9995293744104438,\n",
       "    0.8351138400818386,\n",
       "    0.7801682468760555,\n",
       "    0.8557032564263815,\n",
       "    0.9976900349248995,\n",
       "    0.9998545106737958,\n",
       "    0.9996286420051577,\n",
       "    0.9966461022799771,\n",
       "    0.9995564152575368,\n",
       "    0.9819467454831295,\n",
       "    0.9995614535608586,\n",
       "    0.9930999438404225,\n",
       "    0.8180535460437383,\n",
       "    0.7606066198781425,\n",
       "    0.970328627006491,\n",
       "    0.9793732665387238,\n",
       "    0.9996283705656451,\n",
       "    0.9545936448220521,\n",
       "    0.9654684451389212,\n",
       "    0.9995346962907649,\n",
       "    0.9999992978809753,\n",
       "    0.9818727270055864,\n",
       "    0.9981878147786721,\n",
       "    0.665101091767733,\n",
       "    0.45294984210864786,\n",
       "    0.9733248084167188,\n",
       "    0.9985494460388739,\n",
       "    0.9999781669595598,\n",
       "    0.995024848442217,\n",
       "    0.9993979782803166,\n",
       "    0.7885594617631819,\n",
       "    0.9997398904747675,\n",
       "    0.9999792935409324,\n",
       "    0.9999864452182976,\n",
       "    0.44781182671078196,\n",
       "    0.9329466547463708,\n",
       "    0.9999675698704904,\n",
       "    0.5190131679569014,\n",
       "    0.8806384959789129,\n",
       "    0.9461443943738265,\n",
       "    0.9999701666936248,\n",
       "    0.9966191711837905,\n",
       "    0.9982405115351973,\n",
       "    0.9999201666935423,\n",
       "    0.922675610524789,\n",
       "    0.6977529167917506,\n",
       "    0.6339660816594145,\n",
       "    0.9927408735425562,\n",
       "    0.9960057615102524,\n",
       "    0.9367602855214102,\n",
       "    0.9182218201869405,\n",
       "    0.9966825193376402,\n",
       "    0.9121649877641423,\n",
       "    0.9974867892159075,\n",
       "    0.8843238808152619,\n",
       "    0.9999368073705333,\n",
       "    0.9925383453455635,\n",
       "    0.9999910587527033,\n",
       "    0.7830454193025327,\n",
       "    0.9999116533858834,\n",
       "    0.7501581557100669,\n",
       "    0.9996942712202636,\n",
       "    0.9993117382139077,\n",
       "    0.9969442720813102,\n",
       "    0.9839126619951022,\n",
       "    0.9999926339833181,\n",
       "    0.9766321473433548,\n",
       "    0.9555732218190918,\n",
       "    0.9586073699652646,\n",
       "    0.9996520770558166,\n",
       "    0.9737812208445225,\n",
       "    0.999992589241134,\n",
       "    0.6549549484635974,\n",
       "    0.990392259865807,\n",
       "    0.9130804404515152,\n",
       "    0.9998865102789501,\n",
       "    0.730603367752736,\n",
       "    0.831539963018213,\n",
       "    0.9999613117815841,\n",
       "    0.9984186919233955,\n",
       "    0.9867667066219392,\n",
       "    0.9999754976896176,\n",
       "    0.8511905035071257,\n",
       "    0.9997974522001487,\n",
       "    0.8980597880429881,\n",
       "    0.982047369495544,\n",
       "    0.9999742946991153,\n",
       "    0.9990737831656292,\n",
       "    0.8093327263113701,\n",
       "    0.9684883014705866,\n",
       "    0.9999441542547113,\n",
       "    0.9998711904338266,\n",
       "    0.9991663832772633,\n",
       "    0.9996018288494259,\n",
       "    0.9714908763998865,\n",
       "    0.852717982131881,\n",
       "    0.40657441122601984,\n",
       "    0.9137733078630533,\n",
       "    0.9999668981878107,\n",
       "    0.9966909440655465,\n",
       "    0.8882786569266001,\n",
       "    0.9981298065545439,\n",
       "    0.9969402430679359,\n",
       "    0.6542718215328528,\n",
       "    0.9844667802917354,\n",
       "    0.9876141628329242,\n",
       "    0.9961330079146941,\n",
       "    0.9996890064259141,\n",
       "    0.5164695476655189,\n",
       "    0.9730962556894085,\n",
       "    0.9897936575856565,\n",
       "    0.9985335561523088,\n",
       "    0.9958155486477279,\n",
       "    0.999613050123189,\n",
       "    0.9999731590454521,\n",
       "    0.9909199388779858,\n",
       "    0.9981578210601719,\n",
       "    0.9274239348295364,\n",
       "    0.9800485634067282,\n",
       "    0.9999716815303767,\n",
       "    0.9990922183095942,\n",
       "    0.9977957895559635,\n",
       "    0.9880012009975899,\n",
       "    0.999928008770799,\n",
       "    0.9993384756996001,\n",
       "    0.9999898500107041,\n",
       "    0.484240678575672,\n",
       "    0.9392284744163817,\n",
       "    0.9994670123394331,\n",
       "    0.9320420717609067,\n",
       "    0.9955288488976229,\n",
       "    0.9937378393610474,\n",
       "    0.9839754288062046,\n",
       "    0.9875275331233111,\n",
       "    0.9962560042426176,\n",
       "    0.9997573928931796,\n",
       "    0.9742583917783505,\n",
       "    0.9999110840363273,\n",
       "    0.9994333224413657,\n",
       "    0.9942845644165844,\n",
       "    0.9707868305815435,\n",
       "    0.9999941014381177,\n",
       "    0.934528371375028,\n",
       "    0.9975688483115418,\n",
       "    0.9999645141402368,\n",
       "    0.9492596365104984,\n",
       "    0.9917904033003614,\n",
       "    0.7203987668533004,\n",
       "    0.5249026608063498,\n",
       "    0.8043593567309912,\n",
       "    0.9999851498007893,\n",
       "    0.8260550586757581,\n",
       "    0.9906221725235241,\n",
       "    0.9303042117738484,\n",
       "    0.9973513836503812,\n",
       "    0.9998538043589602,\n",
       "    0.7318566543693164,\n",
       "    0.924359137356025,\n",
       "    0.999912718912543,\n",
       "    0.9999007326985336,\n",
       "    0.9982572331909354,\n",
       "    0.9849629725258353,\n",
       "    0.899808442804017,\n",
       "    0.9992922323167057,\n",
       "    0.9986571119965065,\n",
       "    0.9998334831804174,\n",
       "    0.9253294812975572,\n",
       "    0.8507666507285883,\n",
       "    0.999253011995138,\n",
       "    0.9996154267426645,\n",
       "    0.9920751756443809,\n",
       "    0.9010108264207439,\n",
       "    0.7403053301660127,\n",
       "    0.8831914699740921,\n",
       "    0.9992006068308588,\n",
       "    0.9998147954228592,\n",
       "    0.6933888240550461,\n",
       "    0.9996103924146531,\n",
       "    0.9998485913945055,\n",
       "    0.9999649920277531,\n",
       "    0.9999891790123927,\n",
       "    0.9974817911207243,\n",
       "    0.9593463615565111,\n",
       "    0.9999803108652754,\n",
       "    0.9947953775405078,\n",
       "    0.8188062043007209,\n",
       "    0.8695629647554586,\n",
       "    0.9999695971245895,\n",
       "    0.9714211966711511,\n",
       "    0.9985403260496146,\n",
       "    0.9664294656658263,\n",
       "    0.9916587848597255,\n",
       "    0.9999912417581455,\n",
       "    0.9997798302379449,\n",
       "    0.9794592111666547,\n",
       "    0.898186086206211,\n",
       "    0.9875902867207395,\n",
       "    0.995632737043151,\n",
       "    0.999416718964383,\n",
       "    0.9999283515577637,\n",
       "    0.9997638176408953,\n",
       "    0.9989163026071621,\n",
       "    0.8699129241253198,\n",
       "    0.5607414913350864,\n",
       "    0.9988832074090692,\n",
       "    0.8204198120489447,\n",
       "    0.8505274858313745,\n",
       "    0.9932989246716862,\n",
       "    0.8279514727062383,\n",
       "    0.9759152601001991,\n",
       "    0.7986451275653829,\n",
       "    0.7511788563151609,\n",
       "    0.9999034988262419,\n",
       "    0.8727064484152206,\n",
       "    0.9999408564105978,\n",
       "    0.9959001954263754,\n",
       "    0.9037547109545963,\n",
       "    0.9507993873859967,\n",
       "    0.9982783368125575,\n",
       "    0.9996295193839969,\n",
       "    0.999964897350248,\n",
       "    0.9998598252713684,\n",
       "    0.9019871219146578,\n",
       "    0.9780045857421777,\n",
       "    0.950772832109625,\n",
       "    0.9999918785653138,\n",
       "    0.9999962542086104,\n",
       "    0.9987971793509581,\n",
       "    0.8530515300171702,\n",
       "    0.9756232087022028,\n",
       "    0.7022648663951171,\n",
       "    0.9998760851782599,\n",
       "    0.9999992102434645,\n",
       "    0.9999668123155641,\n",
       "    0.9993481120590543,\n",
       "    0.9774000490074041,\n",
       "    0.9989881900588157,\n",
       "    0.9996376473524687,\n",
       "    0.9999992528584556,\n",
       "    0.9997452858473087,\n",
       "    0.9998311667684575,\n",
       "    0.9954883199101006,\n",
       "    0.9999480579906692,\n",
       "    0.7017866068786887,\n",
       "    0.999987866568249,\n",
       "    0.9728716648278227,\n",
       "    0.9886054316118081,\n",
       "    0.9999929968540742,\n",
       "    0.9989060587502002,\n",
       "    0.9989928475596673,\n",
       "    0.9895237824951852,\n",
       "    0.999937092001444,\n",
       "    0.788822615748702,\n",
       "    0.999908170626583,\n",
       "    0.9997331663727939,\n",
       "    0.9999427014449483,\n",
       "    0.7538596857454214,\n",
       "    0.9989070026543403,\n",
       "    0.9995326957362325,\n",
       "    0.9985926908095989,\n",
       "    0.9956714860203401,\n",
       "    0.9996273000728841,\n",
       "    0.9999836651047216,\n",
       "    0.9995668046238105,\n",
       "    0.9996042057157618,\n",
       "    0.9614045554087025,\n",
       "    0.9999528450832037,\n",
       "    0.9999781660951593,\n",
       "    0.9995875687081934,\n",
       "    0.9424717874414278,\n",
       "    0.999302797399099,\n",
       "    0.9964514928470213,\n",
       "    0.9999424280631103,\n",
       "    0.9999238811576098,\n",
       "    0.9971463200265033,\n",
       "    0.9874052670011123,\n",
       "    0.995315471270194,\n",
       "    0.9999838194599682,\n",
       "    0.8784595132175885,\n",
       "    0.9998450962617855,\n",
       "    0.956624447530224,\n",
       "    0.9999787453362656,\n",
       "    0.7438308768647699,\n",
       "    0.9972349144862556,\n",
       "    0.9950228520768715,\n",
       "    0.9999330618899328,\n",
       "    0.9991081500257536,\n",
       "    0.5240360190751832,\n",
       "    0.989651678761001,\n",
       "    0.9969262359099543,\n",
       "    0.788633377269032,\n",
       "    0.9996984104947485,\n",
       "    0.9998783814788594,\n",
       "    0.9999113249141433,\n",
       "    0.9688391788347324,\n",
       "    0.9995786373961479,\n",
       "    0.9999956514511783,\n",
       "    0.979492665149874,\n",
       "    0.9999208860554403,\n",
       "    0.9989869072609495,\n",
       "    0.9957306470770738,\n",
       "    0.9987243000431353,\n",
       "    0.4987476423123907,\n",
       "    0.9999618830830551,\n",
       "    0.991929100432398,\n",
       "    0.9997880067750157,\n",
       "    0.9769575663986522,\n",
       "    0.99999155891429,\n",
       "    0.9998970886333588,\n",
       "    0.9997949721707933,\n",
       "    0.9999471942778194,\n",
       "    0.9966987440871374,\n",
       "    0.9951298916770802,\n",
       "    0.9946916647007841,\n",
       "    0.999610728218058,\n",
       "    0.988179752789839,\n",
       "    0.9744890640058909,\n",
       "    0.9698094875174857,\n",
       "    0.9896082166348166,\n",
       "    0.9999470280591347,\n",
       "    0.524508267996525,\n",
       "    0.9994314186002007,\n",
       "    0.9954641093811232,\n",
       "    0.9999711688480952,\n",
       "    0.8734143839056979,\n",
       "    0.9982766306416975,\n",
       "    0.9995043242802261,\n",
       "    0.9848938838053374,\n",
       "    0.9870869592940491,\n",
       "    0.9935600853349484,\n",
       "    0.9999639452425082,\n",
       "    0.9969291371571031,\n",
       "    0.9829852312694343,\n",
       "    0.7191834590626265,\n",
       "    0.9999870149282453,\n",
       "    0.6728104019229108,\n",
       "    0.5166193610901639,\n",
       "    0.9965618821326521,\n",
       "    0.8379537338642381,\n",
       "    0.8512758333670466,\n",
       "    0.998811870643171,\n",
       "    0.9984771970965329,\n",
       "    0.957750329672061,\n",
       "    0.9974792150038393,\n",
       "    0.5235134750992603,\n",
       "    0.9990209132982781,\n",
       "    0.9628590254280448,\n",
       "    0.9984824298290591,\n",
       "    0.9794750003772261,\n",
       "    0.9974899731644191,\n",
       "    0.9999927891126781,\n",
       "    0.9940245228342146,\n",
       "    0.9997635200506277,\n",
       "    0.6539374618809338,\n",
       "    0.9999371700756152,\n",
       "    0.9995654275097992,\n",
       "    0.9857318963882645,\n",
       "    0.9562474115271699,\n",
       "    0.6654090127699105,\n",
       "    0.9999927896241688,\n",
       "    0.9999910188100573,\n",
       "    0.9934807653059268,\n",
       "    0.999899172522805,\n",
       "    0.9999357623220586,\n",
       "    0.9985913239746671,\n",
       "    0.7518477286045194,\n",
       "    0.9980425708983428,\n",
       "    0.9971167299101444,\n",
       "    0.9394994136277803,\n",
       "    0.9959677218799827,\n",
       "    0.9988222399651617,\n",
       "    0.9994104103013366,\n",
       "    0.9960969370361974,\n",
       "    0.8116100729767887,\n",
       "    0.996101751597609,\n",
       "    0.9908980248396758,\n",
       "    0.4901689850910149,\n",
       "    0.9313660032911366,\n",
       "    0.9955199109978871,\n",
       "    0.6462832105270436,\n",
       "    0.9940900911734823,\n",
       "    0.9991247895990398,\n",
       "    0.79396865683176,\n",
       "    0.5687031687974607,\n",
       "    0.9998112344463758,\n",
       "    0.5498340289052465,\n",
       "    0.9975701988144349,\n",
       "    0.8702826033767792,\n",
       "    0.9936484301506571,\n",
       "    0.5934945398809662,\n",
       "    0.9996897491762714,\n",
       "    0.9938473598301839,\n",
       "    0.999676425594413,\n",
       "    0.8999274090372892,\n",
       "    0.9992608641301423,\n",
       "    0.999198206578462,\n",
       "    0.9925782009908833,\n",
       "    0.9998670300099316,\n",
       "    0.9943276760918696,\n",
       "    0.789116801253993,\n",
       "    0.9630097446349226,\n",
       "    0.9507579900330151,\n",
       "    0.9855595428240527,\n",
       "    0.9854940828378679,\n",
       "    0.9963761394351731,\n",
       "    0.9999653803207461,\n",
       "    0.999981174682933,\n",
       "    0.42053766098610434,\n",
       "    0.9983730730227502,\n",
       "    0.9656059188124286,\n",
       "    0.7138308808518602,\n",
       "    0.9916296072824903,\n",
       "    0.974917206440325,\n",
       "    0.9968688177022933,\n",
       "    0.9904763423047168,\n",
       "    0.9991867349547455,\n",
       "    0.6258229728739781,\n",
       "    0.966912892125946,\n",
       "    0.42978123687877556,\n",
       "    0.9984429956125315,\n",
       "    0.6430941684967256,\n",
       "    0.9408298480557239,\n",
       "    0.9937978837344572,\n",
       "    0.9853041872404722,\n",
       "    0.9993389752517862,\n",
       "    0.9016482546448629,\n",
       "    0.9998499653748123,\n",
       "    0.9940606619558031,\n",
       "    0.7765867013410858,\n",
       "    0.8621241320488694,\n",
       "    0.9976459636169657,\n",
       "    0.9940361926759747,\n",
       "    0.9977261757697188,\n",
       "    0.9994223064392956,\n",
       "    0.7161501616441156,\n",
       "    0.9969500979324907,\n",
       "    0.988410142852641,\n",
       "    0.9999081610332713,\n",
       "    0.517749298046014,\n",
       "    0.9973579295733028,\n",
       "    0.5275493286589434,\n",
       "    0.9970279335982976,\n",
       "    0.9998106896214358,\n",
       "    0.9141845359701184,\n",
       "    0.9997630621989182,\n",
       "    0.6867791053685275,\n",
       "    0.96484468563729,\n",
       "    0.9962124225149352,\n",
       "    0.9876099250882552,\n",
       "    0.47804084502390254,\n",
       "    0.9995847659706302,\n",
       "    0.9829436000956124,\n",
       "    0.7909417884411425,\n",
       "    0.9996233586509459,\n",
       "    0.9741090970412222,\n",
       "    0.9907423308882155,\n",
       "    0.9984138752602625,\n",
       "    0.9964523634105101,\n",
       "    0.8864701704949572,\n",
       "    0.8581637235730483,\n",
       "    0.999380950449513,\n",
       "    0.99408388092429,\n",
       "    0.8905663828232877,\n",
       "    0.9989606435783542,\n",
       "    0.999948502450702,\n",
       "    0.8026173850152463,\n",
       "    0.9962557002422111,\n",
       "    0.9982124104529972,\n",
       "    0.6912191012779771,\n",
       "    0.999942183638631,\n",
       "    0.9743890234300542,\n",
       "    0.9989398501859692,\n",
       "    0.9993404056906254,\n",
       "    0.9983460844245604,\n",
       "    0.9917486101253551,\n",
       "    0.9995692973950436,\n",
       "    0.9997767403922286,\n",
       "    0.9978937673210513,\n",
       "    0.9919217756005477,\n",
       "    0.9066812226947366,\n",
       "    0.9989316304587781,\n",
       "    0.9988472148913528,\n",
       "    0.9789562213434027,\n",
       "    0.9968956852606483,\n",
       "    0.825927240243424,\n",
       "    0.9999912383579038,\n",
       "    0.9945803902533505,\n",
       "    0.9957110716914184,\n",
       "    0.9996410992911878,\n",
       "    0.9975383715399356,\n",
       "    0.8260102970334802,\n",
       "    0.7885229352982299,\n",
       "    0.5442527596887191,\n",
       "    0.8992161354047942,\n",
       "    0.9998047172709442,\n",
       "    0.9995874265718943,\n",
       "    0.9989503798953225,\n",
       "    0.9808862799616402,\n",
       "    0.9975887681613136,\n",
       "    0.998458232801851,\n",
       "    0.995366948794804,\n",
       "    0.9942237004014063,\n",
       "    0.9898974767501122,\n",
       "    0.99949705300961,\n",
       "    0.9997437141483118,\n",
       "    0.6591127879663646,\n",
       "    0.4974694973935699,\n",
       "    0.9618134036740095,\n",
       "    0.9996692294127577,\n",
       "    0.9588922903704099,\n",
       "    0.9965159014188114,\n",
       "    0.9989916704990887,\n",
       "    0.999502624857575,\n",
       "    0.9946626954362312,\n",
       "    0.5435629392256803,\n",
       "    0.9989916796370035,\n",
       "    0.7656000713730358,\n",
       "    0.9701903692042486,\n",
       "    0.9999540332531095,\n",
       "    0.9859586397230754,\n",
       "    0.9955164154080527,\n",
       "    0.8571093302587501,\n",
       "    0.9987958862861253,\n",
       "    0.9995365596632706,\n",
       "    0.48595377737109996,\n",
       "    0.9881036474736428,\n",
       "    0.9957219413849617,\n",
       "    0.9996024293017836,\n",
       "    0.998947316588927,\n",
       "    0.7034682333367647,\n",
       "    0.988794100724275,\n",
       "    0.9995065229519258,\n",
       "    0.9887905265143935,\n",
       "    0.9996905850137409,\n",
       "    0.6499049505662904,\n",
       "    0.9541938069083424,\n",
       "    0.9908581260485785,\n",
       "    0.9984495082088151,\n",
       "    0.9485466658530957,\n",
       "    0.9930383268953811,\n",
       "    0.9863786077141031,\n",
       "    0.9932841219145159,\n",
       "    0.9984543797662124,\n",
       "    0.9955550995877358,\n",
       "    0.9923939598961453,\n",
       "    0.998334069184171,\n",
       "    0.8885945605139866,\n",
       "    0.9994865586344137,\n",
       "    0.999601361641544,\n",
       "    0.9992664607475156,\n",
       "    0.9975637256226562,\n",
       "    0.7960093468159253,\n",
       "    0.8490640330509642,\n",
       "    0.9998789919511459,\n",
       "    0.99985313239826,\n",
       "    0.9169750818625405,\n",
       "    0.9111504219914746,\n",
       "    0.992192486431976,\n",
       "    0.9980722165208883,\n",
       "    0.9785340677683465,\n",
       "    0.9976628156409069,\n",
       "    0.9655398442302101,\n",
       "    0.9986129345588957,\n",
       "    0.862757102575452,\n",
       "    0.9969599970560605,\n",
       "    0.9995479082611508,\n",
       "    0.9871156876060243,\n",
       "    0.9934454663020378,\n",
       "    0.9780416749894904,\n",
       "    0.9957796884668849,\n",
       "    0.9239701917797832,\n",
       "    0.9889200197648478,\n",
       "    0.9997276923738406,\n",
       "    0.9998263267473543,\n",
       "    0.8172577981409165,\n",
       "    0.996415738494594,\n",
       "    0.8439098949845657,\n",
       "    0.9994001011509286,\n",
       "    0.8569257431054879,\n",
       "    0.9961466143079154,\n",
       "    0.9997149227541391,\n",
       "    0.9991061209988714,\n",
       "    0.9998700760843514,\n",
       "    0.9991010001176306,\n",
       "    0.9997678316058103,\n",
       "    0.9927538572650594,\n",
       "    0.9784338611108309,\n",
       "    0.84175587175109,\n",
       "    0.9997507645334521,\n",
       "    0.9939234573533243,\n",
       "    0.7668414112416095,\n",
       "    0.9992724360197545,\n",
       "    0.9966075155109522,\n",
       "    0.9992566559641627,\n",
       "    0.998440693341168,\n",
       "    0.981590882604399,\n",
       "    0.9787517533539986,\n",
       "    0.9129400229655794,\n",
       "    0.8877155216276782,\n",
       "    0.6969303192009835,\n",
       "    0.9868365156566111,\n",
       "    0.9954989562505405,\n",
       "    0.9556393869239147,\n",
       "    0.9907901834276404,\n",
       "    0.9937577709695434,\n",
       "    0.9998861318465279,\n",
       "    0.9995608867245961,\n",
       "    0.9813886345116357,\n",
       "    0.8172928060066738,\n",
       "    0.9412910844822525,\n",
       "    0.9941239760877643,\n",
       "    0.9981590634082844,\n",
       "    0.9913880012528664,\n",
       "    0.9986639042427222,\n",
       "    0.9995750508561805,\n",
       "    0.9916208359658876,\n",
       "    0.7284292655932806,\n",
       "    0.9996425876622467,\n",
       "    0.9988646349270284,\n",
       "    0.7469997323188908,\n",
       "    0.8000555933090651,\n",
       "    0.9968867956328412,\n",
       "    0.9811047679550068,\n",
       "    0.7793525464003661,\n",
       "    0.995024607085397,\n",
       "    0.9734436602002977,\n",
       "    0.9977564101117687,\n",
       "    0.9999857728818098,\n",
       "    0.9992037288335589,\n",
       "    0.9857447314798747,\n",
       "    0.8035702888035793,\n",
       "    0.9971538036088563,\n",
       "    0.7866504425486135,\n",
       "    0.9999748327676304,\n",
       "    0.9990410255208012,\n",
       "    0.7022311037266387,\n",
       "    0.9819245869520518,\n",
       "    0.9997902054826193,\n",
       "    0.9993421298553031,\n",
       "    0.8866809358744959,\n",
       "    0.5960034928387759,\n",
       "    0.9283847444449628,\n",
       "    0.9598831948988564,\n",
       "    0.9992891634321309,\n",
       "    0.9994897931371225,\n",
       "    0.9951277835483334,\n",
       "    0.9148940996873531,\n",
       "    0.9966409961231889,\n",
       "    0.9969949663795362,\n",
       "    0.9996741134689989,\n",
       "    0.998487415625217,\n",
       "    0.998948746668717,\n",
       "    0.6277699240622517,\n",
       "    0.996892514299297,\n",
       "    0.9988493501702129,\n",
       "    0.48785755183502355,\n",
       "    0.995918596142813,\n",
       "    0.9970964961235842,\n",
       "    0.999801459947684,\n",
       "    0.9938570341124867,\n",
       "    0.9924989031783632,\n",
       "    0.9978605517039978,\n",
       "    0.6862100982066432,\n",
       "    0.9992185381685623,\n",
       "    0.9999298051726386,\n",
       "    0.9996615363332338,\n",
       "    0.9989474921593398,\n",
       "    0.9998911230129549,\n",
       "    0.7260842611105669,\n",
       "    0.9994086611100005,\n",
       "    0.9998253632695411,\n",
       "    0.9997799338051432,\n",
       "    0.9997116327245914,\n",
       "    ...])})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[(0,20000)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(res[(0,20000)][2][1].values())[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in [0,1]:\n",
    "    plt.hist(list(res[(0,20000)][i][1].values())[0][2], bins=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'RBF_moderate'\n",
    "Q=[(0,20000)]\n",
    "probas = [0.1,0.2,0.5]\n",
    "my_method = MyClassifier\n",
    "my_method1 = MyClassifier\n",
    "my_method2= MyClassifier\n",
    "HT = HoeffdingAdaptiveTreeClassifier\n",
    "NB = GaussianNB\n",
    "ARF = ARFClassifier\n",
    "Cl= KMeans\n",
    "m_params = [\n",
    " {  'classifer':HT,\n",
    "  'clustering_method':Cl,\n",
    "  'clustering_params':{'n_init':'auto','n_clusters':4},\n",
    "  'classifier_params':{},\n",
    "  'time_window':20},\n",
    "  {  'classifer':NB,\n",
    "  'clustering_method':Cl,\n",
    "  'clustering_params':{'n_init':'auto','n_clusters':4},\n",
    "  'classifier_params':{},\n",
    "  'time_window':20},\n",
    "  {  'classifer':ARF,\n",
    "  'clustering_method':Cl,\n",
    "  'clustering_params':{'n_init':'auto','n_clusters':4},\n",
    "  'classifier_params':{},\n",
    "  'time_window':20}]\n",
    "methods = [my_method,my_method1,my_method2]\n",
    "res = train_and_evaluate(initial_stream,dataset_name,Q,probas,methods,m_params,['MM_HT','MM_NB',\"MM_ARF\"],Accuracy,1,B=50,K=10,delay=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0,\n",
       "  10000): [('initail_RBF_moderate',\n",
       "   {'MM': ([Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 75.32%],\n",
       "     [0.66,\n",
       "      0.74,\n",
       "      0.78,\n",
       "      0.81,\n",
       "      0.73,\n",
       "      0.74,\n",
       "      0.8,\n",
       "      0.69,\n",
       "      0.71,\n",
       "      0.75,\n",
       "      0.76,\n",
       "      0.76,\n",
       "      0.75,\n",
       "      0.71,\n",
       "      0.77,\n",
       "      0.74,\n",
       "      0.8,\n",
       "      0.7,\n",
       "      0.7,\n",
       "      0.74,\n",
       "      0.69,\n",
       "      0.76,\n",
       "      0.71,\n",
       "      0.79,\n",
       "      0.78,\n",
       "      0.7,\n",
       "      0.77,\n",
       "      0.7,\n",
       "      0.75,\n",
       "      0.83,\n",
       "      0.64,\n",
       "      0.87,\n",
       "      0.74,\n",
       "      0.75,\n",
       "      0.69,\n",
       "      0.66,\n",
       "      0.73,\n",
       "      0.78,\n",
       "      0.69,\n",
       "      0.77,\n",
       "      0.71,\n",
       "      0.8,\n",
       "      0.77,\n",
       "      0.83,\n",
       "      0.76,\n",
       "      0.77,\n",
       "      0.75,\n",
       "      0.84,\n",
       "      0.85,\n",
       "      0.84,\n",
       "      0.8,\n",
       "      0.82,\n",
       "      0.79,\n",
       "      0.79,\n",
       "      0.77,\n",
       "      0.78,\n",
       "      0.68,\n",
       "      0.77,\n",
       "      0.78,\n",
       "      0.77,\n",
       "      0.83,\n",
       "      0.75,\n",
       "      0.87,\n",
       "      0.79,\n",
       "      0.74,\n",
       "      0.73,\n",
       "      0.83,\n",
       "      0.79,\n",
       "      0.82,\n",
       "      0.73,\n",
       "      0.85,\n",
       "      0.76,\n",
       "      0.77,\n",
       "      0.75,\n",
       "      0.79,\n",
       "      0.75,\n",
       "      0.76,\n",
       "      0.7,\n",
       "      0.71,\n",
       "      0.77,\n",
       "      0.83,\n",
       "      0.73,\n",
       "      0.69,\n",
       "      0.7,\n",
       "      0.73,\n",
       "      0.76,\n",
       "      0.78,\n",
       "      0.79,\n",
       "      0.75,\n",
       "      0.72,\n",
       "      0.73,\n",
       "      0.73,\n",
       "      0.73,\n",
       "      0.71,\n",
       "      0.75,\n",
       "      0.69,\n",
       "      0.68,\n",
       "      0.66,\n",
       "      0.7,\n",
       "      0.64])}), ('RBF_moderate_delay_0_10000',\n",
       "   {'MM': ([Accuracy: 73.62%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 73.75%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 73.56%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 73.56%],\n",
       "     [0.62,\n",
       "      0.7,\n",
       "      0.69,\n",
       "      0.76,\n",
       "      0.67,\n",
       "      0.65,\n",
       "      0.66,\n",
       "      0.62,\n",
       "      0.67,\n",
       "      0.69,\n",
       "      0.72,\n",
       "      0.73,\n",
       "      0.72,\n",
       "      0.64,\n",
       "      0.75,\n",
       "      0.63,\n",
       "      0.84,\n",
       "      0.77,\n",
       "      0.74,\n",
       "      0.78,\n",
       "      0.75,\n",
       "      0.76,\n",
       "      0.74,\n",
       "      0.81,\n",
       "      0.75,\n",
       "      0.71,\n",
       "      0.75,\n",
       "      0.74,\n",
       "      0.75,\n",
       "      0.71,\n",
       "      0.73,\n",
       "      0.81,\n",
       "      0.69,\n",
       "      0.71,\n",
       "      0.68,\n",
       "      0.68,\n",
       "      0.64,\n",
       "      0.72,\n",
       "      0.68,\n",
       "      0.75,\n",
       "      0.68,\n",
       "      0.73,\n",
       "      0.79,\n",
       "      0.81,\n",
       "      0.71,\n",
       "      0.72,\n",
       "      0.63,\n",
       "      0.69,\n",
       "      0.72,\n",
       "      0.77,\n",
       "      0.71,\n",
       "      0.7,\n",
       "      0.75,\n",
       "      0.72,\n",
       "      0.69,\n",
       "      0.77,\n",
       "      0.69,\n",
       "      0.79,\n",
       "      0.8,\n",
       "      0.77,\n",
       "      0.71,\n",
       "      0.72,\n",
       "      0.65,\n",
       "      0.69,\n",
       "      0.73,\n",
       "      0.74,\n",
       "      0.83,\n",
       "      0.77,\n",
       "      0.8,\n",
       "      0.68,\n",
       "      0.81,\n",
       "      0.74,\n",
       "      0.82,\n",
       "      0.73,\n",
       "      0.71,\n",
       "      0.84,\n",
       "      0.8,\n",
       "      0.75,\n",
       "      0.74,\n",
       "      0.73,\n",
       "      0.85,\n",
       "      0.79,\n",
       "      0.79,\n",
       "      0.72,\n",
       "      0.82,\n",
       "      0.78,\n",
       "      0.77,\n",
       "      0.84,\n",
       "      0.77,\n",
       "      0.8,\n",
       "      0.77,\n",
       "      0.71,\n",
       "      0.73,\n",
       "      0.72,\n",
       "      0.78,\n",
       "      0.74,\n",
       "      0.72,\n",
       "      0.73,\n",
       "      0.75,\n",
       "      0.72])}), ('RBF_moderate_constant_delay_ssl_0.1_0_10000',\n",
       "   {'MM': ([Accuracy: 76.38%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 75.57%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 76.42%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 76.42%],\n",
       "     [0.61,\n",
       "      0.64,\n",
       "      0.65,\n",
       "      0.67,\n",
       "      0.78,\n",
       "      0.76,\n",
       "      0.67,\n",
       "      0.74,\n",
       "      0.69,\n",
       "      0.78,\n",
       "      0.76,\n",
       "      0.71,\n",
       "      0.74,\n",
       "      0.69,\n",
       "      0.75,\n",
       "      0.79,\n",
       "      0.77,\n",
       "      0.75,\n",
       "      0.72,\n",
       "      0.76,\n",
       "      0.77,\n",
       "      0.76,\n",
       "      0.71,\n",
       "      0.77,\n",
       "      0.77,\n",
       "      0.75,\n",
       "      0.76,\n",
       "      0.67,\n",
       "      0.79,\n",
       "      0.75,\n",
       "      0.81,\n",
       "      0.7,\n",
       "      0.73,\n",
       "      0.74,\n",
       "      0.77,\n",
       "      0.85,\n",
       "      0.81,\n",
       "      0.79,\n",
       "      0.89,\n",
       "      0.91,\n",
       "      0.86,\n",
       "      0.84,\n",
       "      0.76,\n",
       "      0.75,\n",
       "      0.84,\n",
       "      0.75,\n",
       "      0.83,\n",
       "      0.8,\n",
       "      0.76,\n",
       "      0.88,\n",
       "      0.77,\n",
       "      0.77,\n",
       "      0.85,\n",
       "      0.79,\n",
       "      0.78,\n",
       "      0.74,\n",
       "      0.79,\n",
       "      0.83,\n",
       "      0.85,\n",
       "      0.78,\n",
       "      0.76,\n",
       "      0.77,\n",
       "      0.82,\n",
       "      0.79,\n",
       "      0.75,\n",
       "      0.82,\n",
       "      0.8,\n",
       "      0.8,\n",
       "      0.73,\n",
       "      0.7,\n",
       "      0.76,\n",
       "      0.77,\n",
       "      0.77,\n",
       "      0.79,\n",
       "      0.72,\n",
       "      0.74,\n",
       "      0.71,\n",
       "      0.79,\n",
       "      0.77,\n",
       "      0.75,\n",
       "      0.76,\n",
       "      0.77,\n",
       "      0.78,\n",
       "      0.77,\n",
       "      0.75,\n",
       "      0.72,\n",
       "      0.77,\n",
       "      0.73,\n",
       "      0.71])}), ('RBF_moderate_constant_delay_lfs_0.1_0_10000',\n",
       "   {'MM': ([Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 78.40%],\n",
       "     [0.71,\n",
       "      0.7,\n",
       "      0.81,\n",
       "      0.7,\n",
       "      0.77,\n",
       "      0.77,\n",
       "      0.67,\n",
       "      0.78,\n",
       "      0.72,\n",
       "      0.74,\n",
       "      0.83,\n",
       "      0.66,\n",
       "      0.76,\n",
       "      0.74,\n",
       "      0.8,\n",
       "      0.71,\n",
       "      0.77,\n",
       "      0.74,\n",
       "      0.62,\n",
       "      0.77,\n",
       "      0.7,\n",
       "      0.82,\n",
       "      0.75,\n",
       "      0.79,\n",
       "      0.74,\n",
       "      0.74,\n",
       "      0.78,\n",
       "      0.73,\n",
       "      0.87,\n",
       "      0.77,\n",
       "      0.77,\n",
       "      0.69,\n",
       "      0.7,\n",
       "      0.69,\n",
       "      0.79,\n",
       "      0.72,\n",
       "      0.82,\n",
       "      0.7,\n",
       "      0.84,\n",
       "      0.81,\n",
       "      0.79,\n",
       "      0.76,\n",
       "      0.72,\n",
       "      0.84,\n",
       "      0.83,\n",
       "      0.85,\n",
       "      0.77,\n",
       "      0.8,\n",
       "      0.85,\n",
       "      0.83,\n",
       "      0.84,\n",
       "      0.79,\n",
       "      0.81,\n",
       "      0.83,\n",
       "      0.8,\n",
       "      0.79,\n",
       "      0.81,\n",
       "      0.79,\n",
       "      0.81,\n",
       "      0.82,\n",
       "      0.86,\n",
       "      0.79,\n",
       "      0.83,\n",
       "      0.81,\n",
       "      0.87,\n",
       "      0.83,\n",
       "      0.8,\n",
       "      0.88,\n",
       "      0.81,\n",
       "      0.87,\n",
       "      0.79,\n",
       "      0.83,\n",
       "      0.83,\n",
       "      0.83,\n",
       "      0.79,\n",
       "      0.78,\n",
       "      0.74,\n",
       "      0.79,\n",
       "      0.87,\n",
       "      0.82,\n",
       "      0.83,\n",
       "      0.79,\n",
       "      0.81,\n",
       "      0.83,\n",
       "      0.82,\n",
       "      0.76,\n",
       "      0.79,\n",
       "      0.81,\n",
       "      0.8])}), ('RBF_moderate_constant_delay_ssl_0.2_0_10000',\n",
       "   {'MM': ([Accuracy: 76.09%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 76.29%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 76.11%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 76.11%],\n",
       "     [0.69,\n",
       "      0.77,\n",
       "      0.69,\n",
       "      0.73,\n",
       "      0.74,\n",
       "      0.65,\n",
       "      0.76,\n",
       "      0.69,\n",
       "      0.71,\n",
       "      0.76,\n",
       "      0.74,\n",
       "      0.78,\n",
       "      0.74,\n",
       "      0.69,\n",
       "      0.65,\n",
       "      0.72,\n",
       "      0.78,\n",
       "      0.68,\n",
       "      0.82,\n",
       "      0.77,\n",
       "      0.78,\n",
       "      0.75,\n",
       "      0.74,\n",
       "      0.79,\n",
       "      0.8,\n",
       "      0.77,\n",
       "      0.72,\n",
       "      0.71,\n",
       "      0.8,\n",
       "      0.69,\n",
       "      0.77,\n",
       "      0.83,\n",
       "      0.73,\n",
       "      0.8,\n",
       "      0.87,\n",
       "      0.78,\n",
       "      0.85,\n",
       "      0.76,\n",
       "      0.83,\n",
       "      0.77,\n",
       "      0.77,\n",
       "      0.84,\n",
       "      0.75,\n",
       "      0.82,\n",
       "      0.78,\n",
       "      0.75,\n",
       "      0.8,\n",
       "      0.81,\n",
       "      0.8,\n",
       "      0.69,\n",
       "      0.82,\n",
       "      0.81,\n",
       "      0.77,\n",
       "      0.78,\n",
       "      0.74,\n",
       "      0.8,\n",
       "      0.82,\n",
       "      0.82,\n",
       "      0.81,\n",
       "      0.77,\n",
       "      0.81,\n",
       "      0.69,\n",
       "      0.78,\n",
       "      0.73,\n",
       "      0.82,\n",
       "      0.66,\n",
       "      0.72,\n",
       "      0.71,\n",
       "      0.73,\n",
       "      0.83,\n",
       "      0.77,\n",
       "      0.69,\n",
       "      0.78,\n",
       "      0.74,\n",
       "      0.76,\n",
       "      0.79,\n",
       "      0.8,\n",
       "      0.78,\n",
       "      0.72,\n",
       "      0.68])}), ('RBF_moderate_constant_delay_lfs_0.2_0_10000',\n",
       "   {'MM': ([Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 75.46%],\n",
       "     [0.71,\n",
       "      0.8,\n",
       "      0.77,\n",
       "      0.77,\n",
       "      0.79,\n",
       "      0.65,\n",
       "      0.75,\n",
       "      0.75,\n",
       "      0.73,\n",
       "      0.77,\n",
       "      0.74,\n",
       "      0.76,\n",
       "      0.82,\n",
       "      0.74,\n",
       "      0.73,\n",
       "      0.75,\n",
       "      0.77,\n",
       "      0.68,\n",
       "      0.81,\n",
       "      0.77,\n",
       "      0.74,\n",
       "      0.74,\n",
       "      0.76,\n",
       "      0.82,\n",
       "      0.73,\n",
       "      0.68,\n",
       "      0.69,\n",
       "      0.63,\n",
       "      0.68,\n",
       "      0.59,\n",
       "      0.68,\n",
       "      0.75,\n",
       "      0.68,\n",
       "      0.74,\n",
       "      0.8,\n",
       "      0.77,\n",
       "      0.81,\n",
       "      0.72,\n",
       "      0.78,\n",
       "      0.8,\n",
       "      0.79,\n",
       "      0.8,\n",
       "      0.77,\n",
       "      0.77,\n",
       "      0.88,\n",
       "      0.76,\n",
       "      0.85,\n",
       "      0.8,\n",
       "      0.83,\n",
       "      0.83,\n",
       "      0.86,\n",
       "      0.8,\n",
       "      0.79,\n",
       "      0.82,\n",
       "      0.84,\n",
       "      0.84,\n",
       "      0.76,\n",
       "      0.73,\n",
       "      0.77,\n",
       "      0.74,\n",
       "      0.75,\n",
       "      0.74,\n",
       "      0.69,\n",
       "      0.73,\n",
       "      0.79,\n",
       "      0.75,\n",
       "      0.66,\n",
       "      0.69,\n",
       "      0.77,\n",
       "      0.73,\n",
       "      0.75,\n",
       "      0.71,\n",
       "      0.76,\n",
       "      0.77,\n",
       "      0.72,\n",
       "      0.78,\n",
       "      0.7,\n",
       "      0.65,\n",
       "      0.73,\n",
       "      0.8])}), ('RBF_moderate_constant_delay_ssl_0.5_0_10000',\n",
       "   {'MM': ([Accuracy: 71.44%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 71.79%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 71.52%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 71.52%],\n",
       "     [0.63,\n",
       "      0.7,\n",
       "      0.57,\n",
       "      0.67,\n",
       "      0.67,\n",
       "      0.67,\n",
       "      0.67,\n",
       "      0.69,\n",
       "      0.73,\n",
       "      0.7,\n",
       "      0.68,\n",
       "      0.74,\n",
       "      0.7,\n",
       "      0.73,\n",
       "      0.73,\n",
       "      0.65,\n",
       "      0.72,\n",
       "      0.69,\n",
       "      0.71,\n",
       "      0.68,\n",
       "      0.72,\n",
       "      0.81,\n",
       "      0.69,\n",
       "      0.76,\n",
       "      0.8,\n",
       "      0.72,\n",
       "      0.73,\n",
       "      0.76,\n",
       "      0.58,\n",
       "      0.74,\n",
       "      0.63,\n",
       "      0.81,\n",
       "      0.71,\n",
       "      0.78,\n",
       "      0.71,\n",
       "      0.78,\n",
       "      0.81,\n",
       "      0.73,\n",
       "      0.7,\n",
       "      0.79,\n",
       "      0.81,\n",
       "      0.72,\n",
       "      0.68,\n",
       "      0.78,\n",
       "      0.73,\n",
       "      0.62,\n",
       "      0.78,\n",
       "      0.74,\n",
       "      0.65])}), ('RBF_moderate_constant_delay_lfs_0.5_0_10000',\n",
       "   {'MM': ([Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 0.00%,\n",
       "      Accuracy: 73.80%],\n",
       "     [0.63,\n",
       "      0.75,\n",
       "      0.73,\n",
       "      0.65,\n",
       "      0.72,\n",
       "      0.76,\n",
       "      0.72,\n",
       "      0.71,\n",
       "      0.72,\n",
       "      0.7,\n",
       "      0.71,\n",
       "      0.77,\n",
       "      0.72,\n",
       "      0.69,\n",
       "      0.73,\n",
       "      0.7,\n",
       "      0.74,\n",
       "      0.72,\n",
       "      0.68,\n",
       "      0.71,\n",
       "      0.65,\n",
       "      0.79,\n",
       "      0.67,\n",
       "      0.73,\n",
       "      0.76,\n",
       "      0.67,\n",
       "      0.7,\n",
       "      0.81,\n",
       "      0.7,\n",
       "      0.69,\n",
       "      0.76,\n",
       "      0.79,\n",
       "      0.71,\n",
       "      0.79,\n",
       "      0.79,\n",
       "      0.8,\n",
       "      0.82,\n",
       "      0.79,\n",
       "      0.7,\n",
       "      0.83,\n",
       "      0.8,\n",
       "      0.74,\n",
       "      0.76,\n",
       "      0.8,\n",
       "      0.8,\n",
       "      0.77,\n",
       "      0.76,\n",
       "      0.8,\n",
       "      0.71])})]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magr_env",
   "language": "python",
   "name": "magr_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
