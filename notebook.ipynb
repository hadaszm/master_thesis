{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import river\n",
    "import river.datasets as datasets\n",
    "from river import stream\n",
    "from pprint import pprint\n",
    "import os\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "from river import tree\n",
    "from river import evaluate\n",
    "from river import metrics\n",
    "\n",
    "import numpy as np\n",
    "from  streams.stream_section import StreamSection\n",
    "from streams.utils import FL, FU, generate_stream_section\n",
    "from my_datasets.getting_datasets import get_LED,get_Hyperplane,get_Electricity,get_AGRAWL,get_Airlines,get_CoverType,get_RandomRBF\n",
    "from train_and_eval import train_and_evaluate\n",
    "from functools import partial\n",
    "from train_and_eval import train_and_evaluate\n",
    "from river.metrics.accuracy import Accuracy\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from river.metrics import CohenKappa\n",
    "from river.datasets.synth import ConceptDriftStream\n",
    "from tqdm import tqdm\n",
    "\n",
    "# methods\n",
    "from river.dummy import NoChangeClassifier,PriorClassifier\n",
    "from river.cluster import CluStream\n",
    "from river.forest.adaptive_random_forest import ARFClassifier\n",
    "from river.tree.hoeffding_adaptive_tree_classifier import  HoeffdingAdaptiveTreeClassifier\n",
    "from river.naive_bayes import GaussianNB\n",
    "from semisupervised_methods.clustream_and_label import CluserAndLabel\n",
    "from semisupervised_methods.incremental_classifier import IncrementalClassifer\n",
    "from semisupervised_methods.one_nearest_neighbour import oneNNClassifer\n",
    "# from semisupervised_methods.my_method import MyClassifier\n",
    "from river.drift import ADWIN, PageHinkley\n",
    "import operator\n",
    "from semisupervised_methods.ICLC import ICLC\n",
    "from river.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "date_time = now.strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "logging.basicConfig(filename=f'logs\\\\{date_time}.log', filemode='w',\n",
    "                    format='%(asctime)s - %(message)s', level=logging.INFO, datefmt='%d-%b-%y %H:%M:%S')\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m delay_type \u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m     22\u001b[0m methods \u001b[39m=\u001b[39m [incClasif1,incClasif2]\n\u001b[1;32m---> 23\u001b[0m res \u001b[39m=\u001b[39m train_and_evaluate(initial_stream\u001b[39m=\u001b[39;49minitial_stream,  Q\u001b[39m=\u001b[39;49mQ,\n\u001b[0;32m     24\u001b[0m                          probas\u001b[39m=\u001b[39;49mprobas, methods\u001b[39m=\u001b[39;49mmethods, methods_params\u001b[39m=\u001b[39;49mm_params,\n\u001b[0;32m     25\u001b[0m                          methods_name\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mNB07\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     26\u001b[0m                          metric_fun\u001b[39m=\u001b[39;49mCohenKappa, delay_type\u001b[39m=\u001b[39;49mdelay_type, B\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, K\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, delay\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m,logger \u001b[39m=\u001b[39;49m logger)\n",
      "File \u001b[1;32mc:\\Users\\gosia\\Desktop\\studia\\magisterka\\env\\mgr\\train_and_eval.py:338\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(initial_stream, Q, probas, methods, methods_params, methods_name, metric_fun, delay_type, K, B, delay, logger, warm_up_period)\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[39m# train and evaluation part\u001b[39;00m\n\u001b[0;32m    336\u001b[0m     max_length \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\n\u001b[0;32m    337\u001b[0m         np\u001b[39m.\u001b[39mceil(\u001b[39mmax\u001b[39m([\u001b[39mlen\u001b[39m(st\u001b[39m.\u001b[39mstream) \u001b[39mfor\u001b[39;00m st \u001b[39min\u001b[39;00m stream_set])\u001b[39m/\u001b[39mFREQUENCY_OF_PREDICTIONS))\n\u001b[1;32m--> 338\u001b[0m     results_for_q \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39;49mmap(\u001b[39mlambda\u001b[39;49;00m my_stream: train_for_stream(my_stream, methods, methods_params, methods_name,\n\u001b[0;32m    339\u001b[0m                                                                 metric_fun, K, B, warm_up_period, max_length,logger), stream_set)\n\u001b[0;32m    341\u001b[0m     results[q] \u001b[39m=\u001b[39m results_for_q\n\u001b[0;32m    342\u001b[0m \u001b[39mreturn\u001b[39;00m results\n",
      "File \u001b[1;32mc:\\Users\\gosia\\anaconda3\\envs\\mgr2\\lib\\multiprocessing\\pool.py:364\u001b[0m, in \u001b[0;36mPool.map\u001b[1;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m, func, iterable, chunksize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    360\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[0;32m    361\u001b[0m \u001b[39m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[39m    in a list that is returned.\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[1;32m--> 364\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_async(func, iterable, mapstar, chunksize)\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[1;32mc:\\Users\\gosia\\anaconda3\\envs\\mgr2\\lib\\multiprocessing\\pool.py:771\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    769\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n\u001b[0;32m    770\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 771\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "File \u001b[1;32mc:\\Users\\gosia\\anaconda3\\envs\\mgr2\\lib\\multiprocessing\\pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[1;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[0;32m    123\u001b[0m job, i, func, args, kwds \u001b[39m=\u001b[39m task\n\u001b[0;32m    124\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 125\u001b[0m     result \u001b[39m=\u001b[39m (\u001b[39mTrue\u001b[39;00m, func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds))\n\u001b[0;32m    126\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    127\u001b[0m     \u001b[39mif\u001b[39;00m wrap_exception \u001b[39mand\u001b[39;00m func \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m _helper_reraises_exception:\n",
      "File \u001b[1;32mc:\\Users\\gosia\\anaconda3\\envs\\mgr2\\lib\\multiprocessing\\pool.py:48\u001b[0m, in \u001b[0;36mmapstar\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmapstar\u001b[39m(args):\n\u001b[1;32m---> 48\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(\u001b[39mmap\u001b[39;49m(\u001b[39m*\u001b[39;49margs))\n",
      "File \u001b[1;32mc:\\Users\\gosia\\Desktop\\studia\\magisterka\\env\\mgr\\train_and_eval.py:338\u001b[0m, in \u001b[0;36mtrain_and_evaluate.<locals>.<lambda>\u001b[1;34m(my_stream)\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[39m# train and evaluation part\u001b[39;00m\n\u001b[0;32m    336\u001b[0m     max_length \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\n\u001b[0;32m    337\u001b[0m         np\u001b[39m.\u001b[39mceil(\u001b[39mmax\u001b[39m([\u001b[39mlen\u001b[39m(st\u001b[39m.\u001b[39mstream) \u001b[39mfor\u001b[39;00m st \u001b[39min\u001b[39;00m stream_set])\u001b[39m/\u001b[39mFREQUENCY_OF_PREDICTIONS))\n\u001b[1;32m--> 338\u001b[0m     results_for_q \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m my_stream: train_for_stream(my_stream, methods, methods_params, methods_name,\n\u001b[0;32m    339\u001b[0m                                                                 metric_fun, K, B, warm_up_period, max_length,logger), stream_set)\n\u001b[0;32m    341\u001b[0m     results[q] \u001b[39m=\u001b[39m results_for_q\n\u001b[0;32m    342\u001b[0m \u001b[39mreturn\u001b[39;00m results\n",
      "File \u001b[1;32mc:\\Users\\gosia\\Desktop\\studia\\magisterka\\env\\mgr\\train_and_eval.py:258\u001b[0m, in \u001b[0;36mtrain_for_stream\u001b[1;34m(my_stream, methods, methods_params, methods_name, metric_fun, K, B, warm_up_period, max_length, logger)\u001b[0m\n\u001b[0;32m    255\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m Start processing \u001b[39m\u001b[39m{\u001b[39;00mmy_stream\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    256\u001b[0m \u001b[39mfor\u001b[39;00m mi, method \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(methods):\n\u001b[0;32m    257\u001b[0m     \u001b[39m# initilaze method and variables\u001b[39;00m\n\u001b[1;32m--> 258\u001b[0m     m \u001b[39m=\u001b[39m method(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmethods_params[mi])\n\u001b[0;32m    259\u001b[0m     metrics \u001b[39m=\u001b[39m [metric_fun() \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(B\u001b[39m+\u001b[39m\u001b[39m2\u001b[39m)]\n\u001b[0;32m    260\u001b[0m     periodc_metric \u001b[39m=\u001b[39m Rolling(\n\u001b[0;32m    261\u001b[0m         metric_fun(), window_size\u001b[39m=\u001b[39mFREQUENCY_OF_PREDICTIONS)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "led3 = get_LED(0.1,True,3)\n",
    "led5 = get_LED(0.1,True,5)\n",
    "led7 = get_LED(0.1,True,7)\n",
    "st = [(cur_idx, cur_idx, instance[0], instance[1]) for cur_idx, instance in enumerate(led3.take(3750))]\n",
    "st.extend([(cur_idx, cur_idx, instance[0], instance[1]) for cur_idx, instance in enumerate(led5.take(7500))])\n",
    "st.extend([(cur_idx, cur_idx, instance[0], instance[1]) for cur_idx, instance in enumerate(led7.take(8750))])\n",
    "initial_stream = StreamSection('LED_abrupt', st, True)\n",
    "Q=[(0,20000)]\n",
    "probas = [0.1,0.2]  # probabilities of dropping the label\n",
    "HT1 = HoeffdingAdaptiveTreeClassifier\n",
    "ARF1 = ARFClassifier\n",
    "NB1= GaussianNB\n",
    "incClasif2= IncrementalClassifer\n",
    "incClasif1= IncrementalClassifer\n",
    "incClasif3= IncrementalClassifer\n",
    "m_params = [\n",
    "    {'threshold':0.7,\n",
    "  'classifier':NB1,\n",
    "  'params':{}}\n",
    "]\n",
    "delay_type =1\n",
    "methods = [incClasif1,incClasif2]\n",
    "res = train_and_evaluate(initial_stream=initial_stream,  Q=Q,\n",
    "                         probas=probas, methods=methods, methods_params=m_params,\n",
    "                         methods_name=['NB07'],\n",
    "                         metric_fun=CohenKappa, delay_type=delay_type, B=50, K=10, delay=1000,logger = logger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "led3 = get_LED(0.1,True,3)\n",
    "led5 = get_LED(0.1,True,5)\n",
    "led7 = get_LED(0.1,True,7)\n",
    "st = [(cur_idx, cur_idx, instance[0], instance[1]) for cur_idx, instance in enumerate(led3.take(3750))]\n",
    "st.extend([(cur_idx, cur_idx, instance[0], instance[1]) for cur_idx, instance in enumerate(led5.take(7500))])\n",
    "st.extend([(cur_idx, cur_idx, instance[0], instance[1]) for cur_idx, instance in enumerate(led7.take(8750))])\n",
    "initial_stream = StreamSection('LED_abrupt', st, True)\n",
    "Q=[(0,20000)]\n",
    "probas = [0.1,0.2]  # probabilities of dropping the label\n",
    "dummy1 = PriorClassifier\n",
    "dummy2 = NoChangeClassifier\n",
    "incClasif2= IncrementalClassifer\n",
    "incClasif1= IncrementalClassifer\n",
    "m_params = [\n",
    "\n",
    "  {'threshold':0.7,\n",
    "  'classifier':dummy1,\n",
    "  'params':{}},\n",
    "  {'threshold':0.7,\n",
    "  'classifier':dummy2,\n",
    "  'params':{}}\n",
    "]\n",
    "delay_type =1\n",
    "methods = [incClasif1,incClasif2]\n",
    "res = train_and_evaluate(initial_stream=initial_stream,  Q=Q,\n",
    "                         probas=probas, methods=methods, methods_params=m_params,\n",
    "                         methods_name=['Prior','Majority'],\n",
    "                         metric_fun=Accuracy, delay_type=delay_type, B=50, K=10, delay=1000,logger = logger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LED_abrubt_0_20000\n",
      "LED_abrubt_0_20000_constant_delay\n",
      "LED_abrubt_0_20000_constant_delay_ssl_0.1_0_20000\n",
      "LED_abrubt_0_20000_constant_delay_ssl_0.2_0_20000\n"
     ]
    }
   ],
   "source": [
    "led3 = get_LED(0.1,True,3)\n",
    "led5 = get_LED(0.1,True,5)\n",
    "led7 = get_LED(0.1,True,7)\n",
    "st = [(cur_idx, cur_idx, instance[0], instance[1]) for cur_idx, instance in enumerate(led3.take(3750))]\n",
    "st.extend([(cur_idx, cur_idx, instance[0], instance[1]) for cur_idx, instance in enumerate(led5.take(7500))])\n",
    "st.extend([(cur_idx, cur_idx, instance[0], instance[1]) for cur_idx, instance in enumerate(led7.take(8750))])\n",
    "initial_stream = StreamSection('LED_abrubt', st, True)\n",
    "Q=[(0,20000)]\n",
    "probas = [0.1,0.2]  # probabilities of dropping the label\n",
    "my_method = ICLC\n",
    "Cl1= KMeans\n",
    "ARF = ARFClassifier\n",
    "m_params = [\n",
    " {  'classifer':ARF,\n",
    "  'clustering_method':Cl1,\n",
    "  'clustering_params':{},\n",
    "  'classifier_params':{},\n",
    "  'drift_detector': ADWIN,\n",
    "  'nu':100}\n",
    "]\n",
    "methods = [my_method]\n",
    "delay_type = 1 # constant delay\n",
    "\n",
    "# # run the experiment \n",
    "res = train_and_evaluate(initial_stream=initial_stream,  Q=Q,\n",
    "                         probas=probas, methods=methods, methods_params=m_params,\n",
    "                         methods_name=['ICLC'],\n",
    "                         metric_fun=CohenKappa, delay_type=delay_type, B=50, K=10, delay=1000,logger = logger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LED_abrubt_0_20000\n",
      "LED_abrubt_0_20000_constant_delay\n",
      "LED_abrubt_0_20000_constant_delay_ssl_0.1_0_20000\n",
      "LED_abrubt_0_20000_constant_delay_ssl_0.2_0_20000\n"
     ]
    }
   ],
   "source": [
    "led3 = get_LED(0.1,True,3)\n",
    "led5 = get_LED(0.1,True,5)\n",
    "led7 = get_LED(0.1,True,7)\n",
    "st = [(cur_idx, cur_idx, instance[0], instance[1]) for cur_idx, instance in enumerate(led3.take(3750))]\n",
    "st.extend([(cur_idx, cur_idx, instance[0], instance[1]) for cur_idx, instance in enumerate(led5.take(7500))])\n",
    "st.extend([(cur_idx, cur_idx, instance[0], instance[1]) for cur_idx, instance in enumerate(led7.take(8750))])\n",
    "initial_stream = StreamSection('LED_abrupt', st, True)\n",
    "Q=[(0,20000)]\n",
    "probas = [0.1,0.2]  # probabilities of dropping the label\n",
    "clustream = CluserAndLabel\n",
    "\n",
    "m_params = [\n",
    "    {'seed':123}]\n",
    "methods = [clustream]\n",
    "delay_type = 1 # constant delay\n",
    "\n",
    "# # run the experiment \n",
    "res = train_and_evaluate(initial_stream=initial_stream,  Q=Q,\n",
    "                         probas=probas, methods=methods, methods_params=m_params,\n",
    "                         methods_name=['CluStream'],\n",
    "                         metric_fun=CohenKappa, delay_type=delay_type, B=50, K=10, delay=1000,logger = logger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = get_Hyperplane(n_features=10, n_drift_features=10, mag_change= 0.001,\n",
    "                   noise_percentage = 0.01)\n",
    "initial_stream = generate_stream_section(stream,'HyperPlane','synth',start=0,stop=10000)\n",
    "Q=[(0,len(initial_stream.stream))]\n",
    "HT1 = HoeffdingAdaptiveTreeClassifier\n",
    "ARF1 = ARFClassifier\n",
    "NB1= GaussianNB\n",
    "incClasif2= IncrementalClassifer\n",
    "incClasif1= IncrementalClassifer\n",
    "incClasif3= IncrementalClassifer\n",
    "m_params = [\n",
    "\n",
    "  {'threshold':0.7,\n",
    "  'classifier':ARF1,\n",
    "  'params':{'seed':123}},\n",
    "  {'threshold':0.7,\n",
    "  'classifier':HT1,\n",
    "  'params':{'seed':123}},\n",
    "    {'threshold':0.7,\n",
    "  'classifier':NB1,\n",
    "  'params':{}}\n",
    "]\n",
    "delay_type =1\n",
    "methods = [incClasif1,incClasif2,incClasif3]\n",
    "res = train_and_evaluate(initial_stream=initial_stream,  Q=Q,\n",
    "                         probas=probas, methods=methods, methods_params=m_params,\n",
    "                         methods_name=['ARF07','HT07','NB07'],\n",
    "                         metric_fun=Accuracy, delay_type=delay_type, B=50, K=10, delay=500,logger = logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data stream\n",
    "led3 = get_LED(0.1,True,3)\n",
    "led5 = get_LED(0.1,True,5)\n",
    "led7 = get_LED(0.1,True,7)\n",
    "st = [(cur_idx, cur_idx, instance[0], instance[1]) for cur_idx, instance in enumerate(led3.take(3750))]\n",
    "st.extend([(cur_idx, cur_idx, instance[0], instance[1]) for cur_idx, instance in enumerate(led5.take(7500))])\n",
    "st.extend([(cur_idx, cur_idx, instance[0], instance[1]) for cur_idx, instance in enumerate(led7.take(8750))])\n",
    "initial_stream = StreamSection('LED_abrubt', st, True)\n",
    "Q = [(0, 20000)]  # stream secion boundries\n",
    "probas = [0.1,0.2]  # probabilities of dropping the label\n",
    "\n",
    "clustream = CluserAndLabel\n",
    "\n",
    "m_params = [\n",
    "    {'seed':123}]\n",
    "methods = [clustream]\n",
    "delay_type = 1 # constant delay\n",
    "\n",
    "# # run the experiment \n",
    "res = train_and_evaluate(initial_stream=initial_stream,  Q=Q,\n",
    "                         probas=probas, methods=methods, methods_params=m_params,\n",
    "                         methods_name=['CluStream'],\n",
    "                         metric_fun=Accuracy, delay_type=delay_type, B=50, K=10, delay=300,logger = logger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "date_time = now.strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "logging.basicConfig(filename=f'logs\\\\{date_time}.log', filemode='w',\n",
    "                    format='%(asctime)s - %(message)s', level=logging.INFO, datefmt='%d-%b-%y %H:%M:%S')\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "led3 = get_LED(0.1,True,3)\n",
    "led5 = get_LED(0.1,True,5)\n",
    "led7 = get_LED(0.1,True,7)\n",
    "st = [(cur_idx, cur_idx, instance[0], instance[1]) for cur_idx, instance in enumerate(led3.take(3750))]\n",
    "st.extend([(cur_idx, cur_idx, instance[0], instance[1]) for cur_idx, instance in enumerate(led5.take(7500))])\n",
    "st.extend([(cur_idx, cur_idx, instance[0], instance[1]) for cur_idx, instance in enumerate(led7.take(8750))])\n",
    "initial_stream = StreamSection('LED_abrubt', st, True)\n",
    "Q = [(0, 20000)]  # stream secion boundries\n",
    "probas = [0.1,0.2]  # probabilities of dropping the label\n",
    "\n",
    "clustream = CluserAndLabel\n",
    "\n",
    "m_params = [\n",
    "    {'seed':123}]\n",
    "methods = [clustream]\n",
    "delay_type = 1 # constant delay\n",
    "\n",
    "# # run the experiment \n",
    "res = train_and_evaluate(initial_stream=initial_stream,  Q=Q,\n",
    "                         probas=probas, methods=methods, methods_params=m_params,\n",
    "                         methods_name=['CluStream'],\n",
    "                         metric_fun=Accuracy, delay_type=delay_type, B=50, K=10, delay=300,logger = logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'Cover_Type'\n",
    "Q=[(0,20000)]\n",
    "probas = [0.1,0.2,0.5]\n",
    "HT1 = HoeffdingAdaptiveTreeClassifier\n",
    "ARF1 = ARFClassifier\n",
    "NB1= GaussianNB\n",
    "incClasif2= IncrementalClassifer\n",
    "incClasif1= IncrementalClassifer\n",
    "incClasif3= IncrementalClassifer\n",
    "m_params = [\n",
    "\n",
    "  {'threshold':0.9,\n",
    "  'classifier':ARF1,\n",
    "  'params':{'seed':123}},\n",
    "  {'threshold':0.9,\n",
    "  'classifier':HT1,\n",
    "  'params':{'seed':123}},\n",
    "    {'threshold':0.9,\n",
    "  'classifier':NB1,\n",
    "  'params':{}}\n",
    "]\n",
    "methods = [incClasif1,incClasif2,incClasif3]\n",
    "res = train_and_evaluate(initial,dataset_name,Q,probas,methods,m_params,['ARF09','HT09','NB09'],Accuracy,delay_type = 1,B=50,K=10,delay=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mgr2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
